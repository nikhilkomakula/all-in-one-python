{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Neuron with Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that simulates a single neuron with sigmoid activation, and implements backpropagation to update the neuron's weights and bias. The function should take a list of feature vectors, associated true binary labels, initial weights, initial bias, a learning rate, and the number of epochs. The function should update the weights and bias using gradient descent based on the MSE loss, and return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "- input: features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], labels = [1, 0, 0], initial_weights = [0.1, -0.2], initial_bias = 0.0, - learning_rate = 0.1, epochs = 2\n",
    "- output: updated_weights = [0.1036, -0.1425], updated_bias = -0.0167, mse_values =[0.3033, 0.2942]\n",
    "- reasoning: The neuron receives feature vectors and computes predictions using the sigmoid activation. Based on the predictions and true labels, the gradients of MSE loss with respect to weights and bias are computed and used to update the model parameters across epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Learning with Backpropagation\n",
    "\n",
    "This question involves implementing backpropagation for a single neuron in a neural network. The neuron processes inputs and updates parameters to minimize the Mean Squared Error (MSE) between predicted outputs and true labels.\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "- **Forward Pass:**\n",
    "  - Compute the neuron output by calculating the dot product of the weights and input features and adding the bias:\n",
    "  \n",
    "  $\n",
    "  z = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b\n",
    "  $\n",
    "\n",
    "  $\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $\n",
    "\n",
    "- **Loss Calculation (MSE):**\n",
    "  - The Mean Squared Error is used to quantify the error between the neuron's predictions and the actual labels:\n",
    "  \n",
    "  $\n",
    "  MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i)^2\n",
    "  $\n",
    "\n",
    "- **Backward Pass (Gradient Calculation):**\n",
    "  - Compute the gradient of the MSE with respect to each weight and the bias. This involves the partial derivatives of the loss function with respect to the output of the neuron, multiplied by the derivative of the sigmoid function:\n",
    "  \n",
    "  $\n",
    "  \\frac{\\partial MSE}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i) \\sigma'(z_i) x_{ij}\n",
    "  $\n",
    "\n",
    "  $\n",
    "  \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i) \\sigma'(z_i)\n",
    "  $\n",
    "\n",
    "- **Parameter Update:**\n",
    "  - Update each weight and the bias by subtracting a portion of the gradient determined by the learning rate:\n",
    "  \n",
    "  $\n",
    "  w_j = w_j - \\alpha \\frac{\\partial MSE}{\\partial w_j}\n",
    "  $\n",
    "\n",
    "  $\n",
    "  b = b - \\alpha \\frac{\\partial MSE}{\\partial b}\n",
    "  $\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "This process refines the neuron's ability to predict accurately by iteratively adjusting the weights and bias based on the error gradients, optimizing the neural network's performance over multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights: [0.0235, 0.0235]\n",
      "Updated Bias: -0.0277\n",
      "MSE Values: [0.3001, 0.2992, 0.2983, 0.2975, 0.2966, 0.2957, 0.2948, 0.294, 0.2931, 0.2923, 0.2915, 0.2906, 0.2898, 0.289, 0.2882, 0.2874, 0.2866, 0.2858, 0.2851, 0.2843, 0.2836, 0.2828, 0.2821, 0.2814, 0.2807, 0.28, 0.2793, 0.2786, 0.2779, 0.2773, 0.2766, 0.276, 0.2753, 0.2747, 0.2741, 0.2735, 0.2729, 0.2723, 0.2718, 0.2712, 0.2707, 0.2701, 0.2696, 0.2691, 0.2686, 0.2681, 0.2676, 0.2672, 0.2667, 0.2662, 0.2658, 0.2654, 0.2649, 0.2645, 0.2641, 0.2637, 0.2634, 0.263, 0.2626, 0.2623, 0.2619, 0.2616, 0.2612, 0.2609, 0.2606, 0.2603, 0.26, 0.2597, 0.2594, 0.2591, 0.2589, 0.2586, 0.2584, 0.2581, 0.2579, 0.2576, 0.2574, 0.2572, 0.257, 0.2568, 0.2566, 0.2564, 0.2562, 0.256, 0.2558, 0.2557, 0.2555, 0.2553, 0.2552, 0.255, 0.2549, 0.2547, 0.2546, 0.2545, 0.2543, 0.2542, 0.2541, 0.254, 0.2538, 0.2537, 0.2536, 0.2535, 0.2534, 0.2533, 0.2532, 0.2531, 0.253, 0.253, 0.2529, 0.2528, 0.2527, 0.2526, 0.2526, 0.2525, 0.2524, 0.2524, 0.2523, 0.2522, 0.2522, 0.2521, 0.2521, 0.252, 0.252, 0.2519, 0.2519, 0.2518, 0.2518, 0.2517, 0.2517, 0.2516, 0.2516, 0.2516, 0.2515, 0.2515, 0.2515, 0.2514, 0.2514, 0.2514, 0.2513, 0.2513, 0.2513, 0.2512, 0.2512, 0.2512, 0.2512, 0.2511, 0.2511, 0.2511, 0.2511, 0.2511, 0.251, 0.251, 0.251, 0.251, 0.251, 0.2509, 0.2509, 0.2509, 0.2509, 0.2509, 0.2509, 0.2508, 0.2508, 0.2508, 0.2508, 0.2508, 0.2508, 0.2508, 0.2508, 0.2507, 0.2507, 0.2507, 0.2507, 0.2507, 0.2507, 0.2507, 0.2507, 0.2507, 0.2507, 0.2507, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2506, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2505, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2504, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2502, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.2501, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs):\n",
    "    weights = np.array(initial_weights)\n",
    "    bias = initial_bias\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    mse_values = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        z = np.dot(features, weights) + bias\n",
    "        predictions = sigmoid(z)\n",
    "\n",
    "        mse = np.mean((predictions - labels) ** 2)\n",
    "        mse_values.append(round(mse, 4))\n",
    "\n",
    "        # Gradient calculation for weights and bias\n",
    "        errors = predictions - labels\n",
    "        weight_gradients = (2/len(labels)) * np.dot(features.T, errors * predictions * (1 - predictions))\n",
    "        bias_gradient = (2/len(labels)) * np.sum(errors * predictions * (1 - predictions))\n",
    "\n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * weight_gradients\n",
    "        bias -= learning_rate * bias_gradient\n",
    "\n",
    "        # Round weights and bias for output\n",
    "        updated_weights = np.round(weights, 4)\n",
    "        updated_bias = round(bias, 4)\n",
    "\n",
    "    return updated_weights.tolist(), updated_bias, mse_values\n",
    "\n",
    "# Example usage\n",
    "features = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "labels = [0, 1, 1, 0]\n",
    "initial_weights = [0.5, 0.5]\n",
    "initial_bias = 0.5\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n",
    "\n",
    "print(\"Updated Weights:\", updated_weights)\n",
    "print(\"Updated Bias:\", updated_bias)\n",
    "print(\"MSE Values:\", mse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aiop-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
