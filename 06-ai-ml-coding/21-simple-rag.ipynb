{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation provides a simple RAG system with the following features:\n",
    "\n",
    "- Document loading and processing\n",
    "- Text splitting into manageable chunks\n",
    "- Vector embeddings using OpenAI's embedding model\n",
    "- Vector storage using Chroma\n",
    "- Question-answering using LangChain's RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class SimpleRAG:\n",
    "  def __init__(self, documents_path):\n",
    "      \"\"\"\n",
    "      Initialize the RAG system with a path to documents\n",
    "      \"\"\"\n",
    "      self.documents_path = documents_path\n",
    "      self.embeddings = OpenAIEmbeddings()\n",
    "      self.text_splitter = CharacterTextSplitter(\n",
    "          chunk_size=1000,\n",
    "          chunk_overlap=200,\n",
    "          length_function=len\n",
    "      )\n",
    "      \n",
    "  def load_documents(self):\n",
    "      \"\"\"\n",
    "      Load and process documents from the specified path\n",
    "      \"\"\"\n",
    "      documents = []\n",
    "      for file in os.listdir(self.documents_path):\n",
    "          if file.endswith('.txt'):\n",
    "              loader = TextLoader(os.path.join(self.documents_path, file))\n",
    "              documents.extend(loader.load())\n",
    "      \n",
    "      # Split documents into chunks\n",
    "      texts = self.text_splitter.split_documents(documents)\n",
    "      return texts\n",
    "  \n",
    "  def create_vector_store(self, texts):\n",
    "      \"\"\"\n",
    "      Create a vector store from processed documents\n",
    "      \"\"\"\n",
    "      vectorstore = Chroma.from_documents(\n",
    "          documents=texts,\n",
    "          embedding=self.embeddings,\n",
    "          persist_directory=\"./chroma_db\"\n",
    "      )\n",
    "      return vectorstore\n",
    "  \n",
    "  def setup_qa_chain(self, vectorstore):\n",
    "      \"\"\"\n",
    "      Set up the question-answering chain\n",
    "      \"\"\"\n",
    "      qa_chain = RetrievalQA.from_chain_type(\n",
    "          llm=OpenAI(),\n",
    "          chain_type=\"stuff\",\n",
    "          retriever=vectorstore.as_retriever(),\n",
    "          return_source_documents=True\n",
    "      )\n",
    "      return qa_chain\n",
    "  \n",
    "  def query(self, question):\n",
    "      \"\"\"\n",
    "      Query the RAG system\n",
    "      \"\"\"\n",
    "      texts = self.load_documents()\n",
    "      vectorstore = self.create_vector_store(texts)\n",
    "      qa_chain = self.setup_qa_chain(vectorstore)\n",
    "      \n",
    "      result = qa_chain({\"query\": question})\n",
    "      return {\n",
    "          \"answer\": result[\"result\"],\n",
    "          \"source_documents\": result[\"source_documents\"]\n",
    "      }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "  # Make sure to set OPENAI_API_KEY in your environment variables\n",
    "  # Create a directory with some text documents\n",
    "  docs_path = \"./documents\"\n",
    "  \n",
    "  # Initialize and use the RAG system\n",
    "  rag = SimpleRAG(docs_path)\n",
    "  \n",
    "  # Ask a question\n",
    "  question = \"What are the key points in the document?\"\n",
    "  response = rag.query(question)\n",
    "  \n",
    "  print(\"Answer:\", response[\"answer\"])\n",
    "  print(\"\\nSources:\")\n",
    "  for doc in response[\"source_documents\"]:\n",
    "      print(f\"- {doc.metadata['source']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aiop-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
