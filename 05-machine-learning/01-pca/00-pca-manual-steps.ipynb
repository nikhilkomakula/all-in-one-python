{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c895753e",
   "metadata": {
    "id": "c895753e"
   },
   "source": [
    "## PCA : Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca944fae",
   "metadata": {},
   "source": [
    "### Introduction to PCA\n",
    "PCA stands for Principal Component Analysis, and it is a widely used technique in data analysis and machine learning. At its core, PCA is a way to reduce the complexity of high-dimensional data by identifying the most important patterns and trends in the data.\n",
    "\n",
    "Imagine you have a dataset with many variables, such as age, height, weight, income, and education level, and you want to understand how these variables are related to each other. PCA can help you by finding the underlying structure of the data and identifying the key factors that explain most of the variation in the data.\n",
    "\n",
    "To do this, PCA uses linear algebra to transform the data into a new coordinate system that captures the most important information in the data. The new coordinate system is called the principal components, and each principal component is a linear combination of the original variables.\n",
    "\n",
    "By examining the principal components, you can identify the most important patterns in the data and understand how different variables contribute to these patterns. You can also use the principal components to visualize the data in a lower-dimensional space, which can help you identify clusters or groups of similar data points.\n",
    "\n",
    "Overall, PCA is a powerful tool for exploring and analyzing complex datasets, and it can be applied to a wide range of fields, including biology, economics, psychology, and computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15043f67",
   "metadata": {},
   "source": [
    "### PCA Theory\n",
    "PCA is based on the concept of linear algebra, specifically the eigenvalue decomposition of a covariance matrix. In simple terms, the covariance matrix is a measure of the linear relationship between pairs of variables. Note that PCA is applied on centered data.\n",
    "\n",
    "Let X be an n x p matrix representing the centered data, where n is the number of observations and p is the number of variables. The covariance matrix of X is given by:\n",
    "\n",
    "C = (1/n) * X^T * X\n",
    "\n",
    "where ^T denotes the transpose of a matrix. The covariance matrix is a symmetric positive semi-definite matrix, which means that it has p real eigenvalues and p orthogonal eigenvectors.\n",
    "\n",
    "The eigendecomposition of C is given by:\n",
    "\n",
    "C = V * Lambda * V^T\n",
    "\n",
    "where V is a p x p matrix whose columns are the eigenvectors of C, and Lambda is a diagonal matrix whose entries are the corresponding eigenvalues.\n",
    "\n",
    "The eigenvectors in V are sorted in descending order according to their corresponding eigenvalues in Lambda. The first principal component is the linear combination of the variables that corresponds to the eigenvector with the largest eigenvalue. The second principal component is the linear combination that corresponds to the eigenvector with the second largest eigenvalue, and so on.\n",
    "\n",
    "To compute the principal components of the data, we multiply the centered data matrix X by the matrix of eigenvectors V:\n",
    "\n",
    "Y = X * V\n",
    "\n",
    "where Y is the matrix of principal components. Each column of Y represents a principal component, and each row represents an observation.\n",
    "\n",
    "The proportion of variance explained by each principal component is given by its corresponding eigenvalue divided by the sum of all eigenvalues:\n",
    "\n",
    "prop_i = lambda_i / (sum(lambda))\n",
    "\n",
    "where prop_i is the proportion of variance explained by the i-th principal component.\n",
    "\n",
    "PCA can be used for data compression by selecting the top k principal components that explain the most variance in the data. The compressed data can be reconstructed by multiplying the matrix of selected principal components by the transpose of the matrix of eigenvectors:\n",
    "\n",
    "X_hat = Y_k * V^T_k\n",
    "\n",
    "where Y_k is the matrix of the k selected principal components, and V_k is the matrix of the corresponding k eigenvectors. X_hat is the reconstructed data, which should be close to the original data X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9f9a5",
   "metadata": {},
   "source": [
    "### Data Preparation for PCA\n",
    "Suppose we have a dataset with n observations and p variables. Before applying PCA, we need to perform the following data preparation steps:\n",
    "\n",
    "1. Standardization: PCA is sensitive to the scale of the variables, so we need to standardize the data to have zero mean and unit variance. This can be done by subtracting the mean of each variable and dividing by its standard deviation:\n",
    "X_standardized = (X - mean(X)) / std(X) (where X is the original data matrix, mean(X) is the mean vector of X, and std(X) is the standard deviation vector of X)\n",
    "\n",
    "2. Missing value imputation: If the dataset has missing values, we need to impute them before applying PCA. There are different imputation methods that can be used, such as mean imputation, regression imputation, or multiple imputation.\n",
    "\n",
    "3. Outlier detection: Outliers can affect the results of PCA, so it is important to detect and handle them before applying PCA. One way to detect outliers is by computing the Mahalanobis distance of each observation from the mean of the data. Observations with a large Mahalanobis distance are considered outliers.\n",
    "\n",
    "4. Variable selection: If the dataset has a large number of variables, it may be necessary to perform variable selection before applying PCA. This can be done using various methods, such as correlation analysis, mutual information, or feature importance scores.\n",
    "\n",
    "After performing these data preparation steps, we can apply PCA to the standardized data X_standardized to obtain the principal components and perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddc2ea",
   "metadata": {},
   "source": [
    "### Limitations of PCA\n",
    "1. _Linearity assumption:_ PCA assumes that the data is linearly related. If the data has a nonlinear structure, PCA may not be the most appropriate technique.\n",
    "\n",
    "2. _Loss of interpretability: After performing PCA, the principal components may not be directly interpretable in terms of the original variables. This can make it difficult to explain the results to non-technical stakeholders.\n",
    "\n",
    "3. _Sensitivity to outliers:_ PCA is sensitive to outliers, which can distort the results and lead to incorrect conclusions.\n",
    "\n",
    "4. _Sensitivity to scaling:_ PCA is sensitive to the scale of the variables, which can affect the results. It is important to standardize the variables before performing PCA.\n",
    "\n",
    "5. _Difficulty in choosing the number of components:_ Choosing the number of components to retain can be a challenging task. If too few components are retained, important information may be lost. If too many components are retained, the results may be overfit and not generalize well to new data.\n",
    "\n",
    "6. _Correlation-based:_ PCA assumes that variables are linearly correlated with each other. If the variables are not correlated, PCA may not be the most appropriate technique.\n",
    "\n",
    "7. _Lack of robustness:_ PCA is not a robust technique and can be affected by outliers and influential observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25d6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1cefc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_top_k_components(A, k):\n",
    "    # Step 1: Standardize the data\n",
    "    standardized_A = (A - np.mean(A, axis=0)) / np.std(A, axis=0)\n",
    "\n",
    "    # Step 2: Compute the covariance matrix\n",
    "    covariance_matrix = np.cov(standardized_A, rowvar=False)\n",
    "\n",
    "    # Step 3: Perform eigen decomposition of the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # Step 4: Sort eigenvectors by eigenvalues in descending order\n",
    "    print(eigenvalues)\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    print(sorted_indices)\n",
    "    sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "    print(sorted_eigenvalues)\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    print(eigenvectors)\n",
    "    print(sorted_eigenvectors)\n",
    "\n",
    "    # Step 5: Select the top k eigenvectors\n",
    "    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n",
    "\n",
    "    # Step 6: Project the centered data onto the top k eigenvectors\n",
    "    pca_projection = np.dot(standardized_A, top_k_eigenvectors)\n",
    "\n",
    "    return pca_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d3bea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[0.5488135  0.71518937 0.60276338 0.54488318 0.4236548 ]\n",
      " [0.64589411 0.43758721 0.891773   0.96366276 0.38344152]\n",
      " [0.79172504 0.52889492 0.56804456 0.92559664 0.07103606]\n",
      " [0.0871293  0.0202184  0.83261985 0.77815675 0.87001215]\n",
      " [0.97861834 0.79915856 0.46147936 0.78052918 0.11827443]\n",
      " [0.63992102 0.14335329 0.94466892 0.52184832 0.41466194]\n",
      " [0.26455561 0.77423369 0.45615033 0.56843395 0.0187898 ]\n",
      " [0.6176355  0.61209572 0.616934   0.94374808 0.6818203 ]\n",
      " [0.3595079  0.43703195 0.6976312  0.06022547 0.66676672]\n",
      " [0.67063787 0.21038256 0.1289263  0.31542835 0.36371077]]\n",
      "[2.55533375 1.4114298  0.70162366 0.4000991  0.48706926]\n",
      "[0 1 2 4 3]\n",
      "[2.55533375 1.4114298  0.70162366 0.48706926 0.4000991 ]\n",
      "[[-0.48778004 -0.21458486  0.69108706 -0.38801997 -0.29641719]\n",
      " [-0.51062694 -0.0215367  -0.69789734 -0.4815427  -0.1408979 ]\n",
      " [ 0.38522227 -0.58037433 -0.16914554  0.06571509 -0.6941475 ]\n",
      " [-0.16421402 -0.78441966 -0.0478546   0.10696059  0.58650568]\n",
      " [ 0.57093635 -0.03661883  0.06661604 -0.77575646  0.2577886 ]]\n",
      "[[-0.48778004 -0.21458486  0.69108706 -0.29641719 -0.38801997]\n",
      " [-0.51062694 -0.0215367  -0.69789734 -0.1408979  -0.4815427 ]\n",
      " [ 0.38522227 -0.58037433 -0.16914554 -0.6941475   0.06571509]\n",
      " [-0.16421402 -0.78441966 -0.0478546   0.58650568  0.10696059]\n",
      " [ 0.57093635 -0.03661883  0.06661604  0.2577886  -0.77575646]]\n",
      "Top 2 principal components:\n",
      "[[-0.39261733  0.29683052]\n",
      " [ 0.11966095 -1.66065313]\n",
      " [-1.54158129 -0.82686272]\n",
      " [ 3.10549832 -0.53880078]\n",
      " [-2.4392586  -0.34329219]\n",
      " [ 1.12934083 -0.53357561]\n",
      " [-1.07743577  0.89967341]\n",
      " [ 0.02057124 -0.94102885]\n",
      " [ 1.4980055   1.56637557]\n",
      " [-0.42218385  2.08133379]]\n"
     ]
    }
   ],
   "source": [
    "# Generate a random matrix A\n",
    "np.random.seed(0)\n",
    "A = np.random.rand(10, 5)  # 10 samples, 5 features\n",
    "print(\"A:\", A)\n",
    "\n",
    "# Call the function to get the top k principal components\n",
    "k = 2\n",
    "top_k_components = pca_top_k_components(A, k)\n",
    "print(\"Top\", k, \"principal components:\")\n",
    "print(top_k_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f76bf",
   "metadata": {},
   "source": [
    "Once we have the top k eigenvectors from step 5, these eigenvectors define a new basis in the high-dimensional space of the original data. Each eigenvector represents a direction in this space that captures the most variance in the data.\n",
    "\n",
    "To obtain the principal components, we take the standardized data matrix and project it onto these top k eigenvectors. This projection effectively transforms the data from its original high-dimensional space to a new space defined by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ffacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
