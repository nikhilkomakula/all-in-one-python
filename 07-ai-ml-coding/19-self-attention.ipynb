{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention is a key mechanism in transformer models that allows the model to weigh the importance of different words in a sequence when encoding a particular word. Here's a breakdown of how self-attention is computed in transformers:\n",
    "\n",
    "1\\. **Input Representation**\n",
    "\n",
    "Each input word (or token) is first converted into a <ins>vector representation</ins>, typically using embeddings. For a sequence of words, this results in a matrix $X$ of shape $(n, d)$, where $n$ is the number of tokens and $d$ is the dimensionality of the embeddings.\n",
    "\n",
    "2\\. **Linear Transformations**\n",
    "\n",
    "The input matrix $X$ is transformed into three different matrices: **Queries (Q)**, **Keys (K)**, and **Values (V)**. This is done using <ins>learned weight matrices</ins>:\n",
    "\n",
    "- **Queries**: $Q = XW_Q$\n",
    "- **Keys**: $K = XW_K$\n",
    "- **Values**: $V = XW_V$\n",
    "\n",
    "Here, $W_Q$, $W_K$, and $W_V$ are weight matrices of shape $(d, d_k)$, $(d, d_k)$, and $(d, d_v)$ respectively, where $d_k$ and $d_v$ are the dimensions of the keys and values.\n",
    "\n",
    "3\\. **Compute Attention Scores**\n",
    "\n",
    "The attention scores are computed by taking the <ins>dot product of the queries with the keys</ins>:\n",
    "\n",
    "$$\n",
    "\\text{Attention Scores} = QK^T\n",
    "$$\n",
    "\n",
    "This results in a matrix of shape $(n, n)$, where each element $(i, j)$ represents the attention score of the $i$\\-th token with respect to the $j$\\-th token.\n",
    "\n",
    "4\\. **Scale the Scores**\n",
    "\n",
    "<ins>To prevent the dot products from growing too large</ins> (which can lead to gradients that are too small), the scores are <ins>scaled</ins> by the <ins>square root of the dimension of the keys</ins>:\n",
    "\n",
    "$$\n",
    "\\text{Scaled Scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "5\\. **Apply Softmax**\n",
    "\n",
    "The scaled scores are then passed through a <ins>softmax</ins> function to obtain the <ins>attention weights</ins>. This converts the scores into a probability distribution:\n",
    "\n",
    "$$\n",
    "\\text{Attention Weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "6\\. **Compute the Output**\n",
    "\n",
    "Finally, the output of the self-attention layer is computed by <ins>multiplying the attention weights with the values</ins>:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Attention Weights} \\cdot V\n",
    "$$\n",
    "\n",
    "This results in a new matrix of shape $(n, d_v)$, which is the weighted sum of the values based on the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    Q = np.dot(X, W_q)\n",
    "    K = np.dot(X, W_k)\n",
    "    V = np.dot(X, W_v)\n",
    "    return Q, K, V\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = K.shape[1]                                    # REMEMBER K.shape = K: (batch_size, d_k) (same as Q.shape)\n",
    "    attention_scores = np.matmul(Q, K.T)                # REMEMBER `np.matmul`, and NOT `np.dot`\n",
    "    scaled_attention_scores = attention_scores / np.sqrt(d_k)\n",
    "    attention_weights = np.exp(scaled_attention_scores) / np.sum(np.exp(scaled_attention_scores), axis=1, keepdims=True)\n",
    "    attention_output = np.matmul(attention_weights, V)  # REMEMBER `np.matmul`, and NOT `np.dot`\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6604769 2.6604769]\n",
      " [2.3395231 3.3395231]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aiop-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
