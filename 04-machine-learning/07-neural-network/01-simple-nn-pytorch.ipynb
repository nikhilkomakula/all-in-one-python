{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide explains how to create and train a simple feedforward neural network using PyTorch. The network consists of one hidden layer and uses ReLU activation. This example is a basic introduction to neural networks in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115398c30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch: The core PyTorch library for tensor operations.\n",
    "- torch.nn: Contains modules and classes for building neural networks.\n",
    "- torch.optim: Provides optimization algorithms like Adam, SGD, etc.\n",
    "- torch.manual_seed(42): Sets the random seed to ensure that results are reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "      super(SimpleNN, self).__init__()\n",
    "      self.hidden = nn.Linear(input_size, hidden_size)\n",
    "      self.relu = nn.ReLU()\n",
    "      self.output = nn.Linear(hidden_size, output_size)\n",
    "  \n",
    "  def forward(self, x):\n",
    "      x = self.hidden(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.output(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SimpleNN: A class that defines the neural network architecture.\n",
    "    - init: Initializes the network layers.\n",
    "        - nn.Linear(input_size, hidden_size): Defines a fully connected layer from input to hidden layer.\n",
    "        - nn.ReLU(): Applies the ReLU activation function.\n",
    "        - nn.Linear(hidden_size, output_size): Defines a fully connected layer from hidden to output layer.\n",
    "    - forward: Defines the forward pass of the network, specifying how data flows through the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters and Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network parameters\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = SimpleNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_size: Number of input features.\n",
    "- hidden_size: Number of neurons in the hidden layer.\n",
    "- output_size: Number of output features.\n",
    "- model: An instance of the SimpleNN class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- criterion: The loss function used to measure the difference between the predicted and actual values. Here, Mean Squared Error (MSE) is used.\n",
    "- optimizer: The optimization algorithm used to update the model's weights. Adam is chosen for its efficiency and adaptability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some dummy data\n",
    "X = torch.randn(100, input_size)\n",
    "y = torch.randn(100, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X: Randomly generated input data with 100 samples, each having `input_size` features.\n",
    "- y: Randomly generated target data with 100 samples, each having `output_size` features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.7376\n",
      "Epoch [20/100], Loss: 0.6285\n",
      "Epoch [30/100], Loss: 0.5232\n",
      "Epoch [40/100], Loss: 0.4220\n",
      "Epoch [50/100], Loss: 0.3297\n",
      "Epoch [60/100], Loss: 0.2574\n",
      "Epoch [70/100], Loss: 0.2041\n",
      "Epoch [80/100], Loss: 0.1678\n",
      "Epoch [90/100], Loss: 0.1422\n",
      "Epoch [100/100], Loss: 0.1259\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "  # Forward pass\n",
    "  outputs = model(X)\n",
    "  loss = criterion(outputs, y)\n",
    "  \n",
    "  # Backward pass and optimization\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  # Print progress\n",
    "  if (epoch + 1) % 10 == 0:\n",
    "      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_epochs: Number of times the entire dataset is passed through the network.\n",
    "- Forward pass: Computes the output of the network.\n",
    "- Loss computation: Calculates the loss using the criterion.\n",
    "- Backward pass: Computes the gradient of the loss with respect to the model parameters.\n",
    "- Optimizer step: Updates the model parameters based on the computed gradients.\n",
    "- Progress printing: Outputs the loss every 10 epochs to monitor training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **`optimizer.zero_grad()`**:\n",
    "   - **Purpose**: This line resets the gradients of all model parameters before computing the new gradients.\n",
    "   - **Why it's needed**: In PyTorch, gradients are accumulated by default. This means that if you don't reset them, the gradients from the previous batch will be added to the gradients of the current batch, leading to incorrect updates. By zeroing the gradients, you ensure that each batch's gradients are computed independently.\n",
    "\n",
    "2. **`loss.backward()`**:\n",
    "   - **Purpose**: This line computes the gradient of the loss with respect to each parameter (weight and bias) in the model using backpropagation.\n",
    "   - **How it works**: PyTorch automatically computes the gradients by following the chain rule of calculus. It traverses the computation graph from the loss backward to each parameter, calculating the partial derivatives of the loss with respect to each parameter.\n",
    "   - **Result**: After this call, each parameter in the model has its `.grad` attribute populated with the gradient of the loss with respect to that parameter.\n",
    "\n",
    "3. **`optimizer.step()`**:\n",
    "   - **Purpose**: This line updates the model parameters using the gradients computed in the previous step.\n",
    "   - **How it works**: The optimizer adjusts each parameter by a small amount in the direction that reduces the loss. The specific adjustment depends on the optimization algorithm being used (e.g., Adam, SGD). For instance, in the case of the Adam optimizer, it uses the gradients along with running averages of past gradients and squared gradients to compute the parameter updates.\n",
    "   - **Effect**: This step effectively moves the model parameters in the direction that minimizes the loss, iteratively improving the model's performance on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input: tensor([[ 0.3649,  1.0394, -0.9494,  0.3799,  0.2059, -1.2399,  0.5463, -0.4714,\n",
      "          1.1313,  1.8746]])\n",
      "Model prediction: tensor([[1.7288, 2.0155]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_input = torch.randn(1, input_size)\n",
    "prediction = model(test_input)\n",
    "print(\"Test input:\", test_input)\n",
    "print(\"Model prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test_input: A single random input sample for testing the model.\n",
    "- prediction: The model's output for the test input, demonstrating how to use the trained model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.7376\n",
      "Epoch [20/100], Loss: 0.6285\n",
      "Epoch [30/100], Loss: 0.5232\n",
      "Epoch [40/100], Loss: 0.4220\n",
      "Epoch [50/100], Loss: 0.3297\n",
      "Epoch [60/100], Loss: 0.2574\n",
      "Epoch [70/100], Loss: 0.2041\n",
      "Epoch [80/100], Loss: 0.1678\n",
      "Epoch [90/100], Loss: 0.1422\n",
      "Epoch [100/100], Loss: 0.1259\n",
      "Training complete!\n",
      "Test input: tensor([[ 0.3649,  1.0394, -0.9494,  0.3799,  0.2059, -1.2399,  0.5463, -0.4714,\n",
      "          1.1313,  1.8746]])\n",
      "Model prediction: tensor([[1.7288, 2.0155]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "      super(SimpleNN, self).__init__()\n",
    "      self.hidden = nn.Linear(input_size, hidden_size)\n",
    "      self.relu = nn.ReLU()\n",
    "      self.output = nn.Linear(hidden_size, output_size)\n",
    "  \n",
    "  def forward(self, x):\n",
    "      x = self.hidden(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.output(x)\n",
    "      return x\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define network parameters\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Generate some dummy data\n",
    "X = torch.randn(100, input_size)\n",
    "y = torch.randn(100, output_size)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "  # Forward pass\n",
    "  outputs = model(X)\n",
    "  loss = criterion(outputs, y)\n",
    "  \n",
    "  # Backward pass and optimization\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  # Print progress\n",
    "  if (epoch + 1) % 10 == 0:\n",
    "      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Test the model\n",
    "test_input = torch.randn(1, input_size)\n",
    "prediction = model(test_input)\n",
    "print(\"Test input:\", test_input)\n",
    "print(\"Model prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aiop-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
