{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RAG (HF + Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Splitting PDF...\n",
      "Creating Vector Store...\n",
      "Initlializing LLM...\n",
      "Creating QA Chain...\n",
      "====================================================================================================\n",
      "Questions: What is the difference between PRE and POST operations in OpenPages Triggers?\n",
      "Answer:  In OpenPages Triggers, PRE operations refer to the phase before the execution of a method, while POST operations refer to the phase after the execution of a method. PRE triggers are used for validating data, setting new values, and performing calculations before the data is persisted, while POST triggers are used for updating related objects, creating, moving, copying, or locking other objects based on the current object in context. Additionally, due to the parallel processing capabilities introduced in OpenPages v7.1, it may be necessary to configure trigger executions to occur serially to ensure consistent behavior.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "\n",
    "# Step 1: Load the PDF document\n",
    "def load_pdf(pdf_path):\n",
    "    print(\"Loading PDF...\")\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Step 2: Split the document into chunks\n",
    "def split_documents(documents):\n",
    "    print(\"Splitting PDF...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Create embeddings and vector store\n",
    "def get_vector_store(pdf_path):\n",
    "    \n",
    "    # Initialize embeddings for vector store\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    \n",
    "    # Check if vector store exists\n",
    "    persist_directory = \"./chroma_db\"\n",
    "    if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "        print(\"Loading existing Vector Store...\")\n",
    "        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    else:\n",
    "        # Load and process the PDF\n",
    "        documents = load_pdf(pdf_path)\n",
    "        chunks = split_documents(documents)\n",
    "        \n",
    "        print(\"Creating Vector Store...\")\n",
    "        # Create vector store\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=\"./chroma_db\"\n",
    "        )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Step 4: Initialize the LLM using HuggingFacePipeline\n",
    "def initialize_llm():\n",
    "    print(\"Initlializing LLM...\")\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.2,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "# Step 5: Create the RAG pipeline\n",
    "def create_rag_pipeline(vectorstore, llm):\n",
    "    print(\"Creating QA Chain...\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "# Step 6: Main function to run the RAG system\n",
    "def main(pdf_path, query):\n",
    "        \n",
    "    # Get Vector Store\n",
    "    vectorstore = get_vector_store(pdf_path)\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = initialize_llm()\n",
    "    \n",
    "    # Create RAG pipeline\n",
    "    qa_chain = create_rag_pipeline(vectorstore, llm)\n",
    "    \n",
    "    # Query the system\n",
    "    response = qa_chain.invoke({\"query\": query})\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response[\"result\"],\n",
    "        \"source_documents\": response[\"source_documents\"]\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"Trigger_Developer_Guide.pdf\"\n",
    "    query = \"What is the difference between PRE and POST operations in OpenPages Triggers?\"\n",
    "    \n",
    "    response = main(pdf_path, query)\n",
    "    print(\"=\"*100)\n",
    "    print(\"Questions:\", query)\n",
    "    print(\"Answer:\", response[\"answer\"])\n",
    "    print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aiop-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
